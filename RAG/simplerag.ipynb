{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edf841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA INGESTION\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bac55068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Everything you thought had meaning. \\nEvery hope, dream or moment of happiness. \\nNone of it matters As you lie bleeding out on the battlefield. \\nNone of it changes. \\nWhat a speeding rock does to our body. \\nWe all die. Does that mean our lives are meaningless? \\nDoes that mean that there was no point in being born? \\nWould you say that of our slain comrades? \\nWhat about their lives? \\nWere they meaningless? \\nThey were not. \\nTheir memory serves an example to us all the greatest, the anguished father. \\nTheir lives have meaning because we the living refused to forget them. \\nAnd as we ride to certain death, we trust our successors to do the same for us. \\nBecause my soldiers do not buckle our yield. When faced with a gorilla to be of this world, my soldiers pushed forward. \\nMy soldiers scream out. My soldiers rage.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b584de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7cd3c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#   Web Based Loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_ = (\"post-title\",\"post_content\",\"post-header\")\n",
    "                       )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f7d49ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='lilianweng.github.io', port=443): Max retries exceeded with url: /posts/2023-06-23-agent/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027AE1377A10>: Failed to resolve 'lilianweng.github.io' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mgaierror\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, label empty or too long\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     61\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\socket.py:977\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    976\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    978\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[31mgaierror\u001b[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameResolutionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\urllib3\\connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    752\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\urllib3\\connection.py:205\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameResolutionError\u001b[39m: <urllib3.connection.HTTPSConnection object at 0x0000027AE1377A10>: Failed to resolve 'lilianweng.github.io' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='lilianweng.github.io', port=443): Max retries exceeded with url: /posts/2023-06-23-agent/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027AE1377A10>: Failed to resolve 'lilianweng.github.io' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m web_documents = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\web_base.py:375\u001b[39m, in \u001b[36mWebBaseLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Lazy load text from the url(s) in web_path.\"\"\"\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.web_paths:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     soup = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     text = soup.get_text(**\u001b[38;5;28mself\u001b[39m.bs_get_text_kwargs)\n\u001b[32m    377\u001b[39m     metadata = _build_metadata(soup, path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\web_base.py:357\u001b[39m, in \u001b[36mWebBaseLoader._scrape\u001b[39m\u001b[34m(self, url, parser, bs_kwargs)\u001b[39m\n\u001b[32m    353\u001b[39m         parser = \u001b[38;5;28mself\u001b[39m.default_parser\n\u001b[32m    355\u001b[39m \u001b[38;5;28mself\u001b[39m._check_parser(parser)\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m html_doc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequests_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raise_for_status:\n\u001b[32m    359\u001b[39m     html_doc.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\requests\\sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhij\\OneDrive\\Desktop\\GitHub\\GenAI_Practice\\.venv\\Lib\\site-packages\\requests\\adapters.py:677\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    674\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    675\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPSConnectionPool(host='lilianweng.github.io', port=443): Max retries exceeded with url: /posts/2023-06-23-agent/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027AE1377A10>: Failed to resolve 'lilianweng.github.io' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "web_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc3150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52951f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "loader = PyMuPDFLoader('Resume (26).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e7803ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}, page_content='Abhijeeth Chandragi\\nData Science Specialist — Competitive Programmer\\n# abhijeethchandragi@gmail.com | \\x83 +91 888-521-2806 | + Warangal, Telangana\\nï LinkedIn | § GitHub | Ð CodeChef: 4-star (Max 1863)\\nEducation\\nIndian Institute of Information Technology (IIIT), Nagpur\\nAug 2023 – May 2027\\nB.Tech in Data Science\\nCGPA: 8.24/10.0\\nSri Chaitanya Junior College, Telangana\\nApr 2021 – Mar 2023\\nClass 12th – Telangana State Board\\nPercentage: 98%\\nAryabhatta High School, Telangana\\nJun 2020 – Mar 2021\\nClass 10th – Telangana State Board\\nCGPA: 10.0\\nAchievements & Honors\\n• Amazon ML Summer School 2025 – Selected in the Top 2.3% nationwide (3,000 / 130,000+ applicants); completed\\n4-week intensive training on Machine Learning, Deep Learning, and Generative AI.\\n• CodeChef – 4-Star Coder – Max Rating: 1863 (Top 5% globally)\\n• Global Rank #9 in Div2 rated contest\\n• Top Finishes: #152 in Div2 rated contest #156 in Div2 rated contest\\n• Codeforces – Specialist Tier – Max Rating: 1500 (Top 15% globally)\\n• Top Finishes: #900 (Top 3.6%) in Div2 (25,000+ participants) – #982 (Top 3.9%) in Div3 (25,000+ participants)\\n• Solved 700+ problems across 100+ contests on CodeChef, Codeforces, and other platforms.\\n• HackIndia 2025 Finalist – Secured Top 12 rank (Regional Round) among 500+ competing teams.\\nProjects\\nSmart Document Analyzer | § Code | Flask | Ollama (LLaMA3) | PyMuPDF | Sentence-Transformers\\nMay 2025\\n• Developed a document analysis backend with semantic Q&A, heading-aware chunking, and keyword-enhanced retrieval.\\n• Achieved 95%+ visual alignment by linking images to headings using layout metadata.\\n• Optimized cosine similarity and LLM prompting to reduce query latency by 30%.\\nAcademic Research Assistant | § Code | Flask | SerpAPI | LLaMA3 | MiniLM | PyPDF2\\nApr 2025\\n• Built an AI assistant to automate academic search, summarization, and metadata extraction for 200+ papers.\\n• Implemented SerpAPI for document retrieval and applied LLaMA3 + regex for structured metadata extraction.\\n• Enabled semantic ranking and summarization, reducing document review time by 60%.\\nTechnical Skills\\nProgramming Languages:\\nPython, SQL, JavaScript, C++\\nMachine Learning & AI:\\nPyTorch, TensorFlow, scikit-learn, Deep Learning, NLP, LLMs, RAG Systems\\nData Engineering:\\nPandas, NumPy, Data Preprocessing, ETL Pipelines, Power BI\\nDevelopment & Tools:\\nVS Code, Git & GitHub, Jupyter Notebooks, Linux (Ubuntu, Kali)\\nDatabases:\\nPostgreSQL, MySQL, MongoDB\\nCore Subjects:\\nData Structures & Algorithms, Computer Networks, Operating Systems, DBMS, OOP\\nPositions of Responsibility\\nCore Member: Dotslash Competitive Programming Team Core member, IIIT Nagpur\\nAugust 2025 – Present')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "456e3cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abhijeeth Chandragi\n",
      "Data Science Specialist — Competitive Programmer\n",
      "# abhijeethchandragi@gmail.com |  +91 888-521-2806 | + Warangal, Telangana\n",
      "ï LinkedIn | § GitHub | Ð CodeChef: 4-star (Max 1863)\n",
      "E\n",
      "{'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}\n",
      "------\n",
      "• Global Rank #9 in Div2 rated contest\n",
      "• Top Finishes: #152 in Div2 rated contest #156 in Div2 rated contest\n",
      "• Codeforces – Specialist Tier – Max Rating: 1500 (Top 15% globally)\n",
      "• Top Finishes: #900 (\n",
      "{'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}\n",
      "------\n",
      "• Optimized cosine similarity and LLM prompting to reduce query latency by 30%.\n",
      "Academic Research Assistant | § Code | Flask | SerpAPI | LLaMA3 | MiniLM | PyPDF2\n",
      "Apr 2025\n",
      "• Built an AI assistant to au\n",
      "{'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}\n",
      "------\n",
      "Databases:\n",
      "PostgreSQL, MySQL, MongoDB\n",
      "Core Subjects:\n",
      "Data Structures & Algorithms, Computer Networks, Operating Systems, DBMS, OOP\n",
      "Positions of Responsibility\n",
      "Core Member: Dotslash Competitive Program\n",
      "{'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Initialize RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,     # size of each chunk\n",
    "    chunk_overlap=200,   # overlap between chunks\n",
    "    length_function=len, # function to measure text length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # hierarchy of split rules\n",
    ")\n",
    "\n",
    "# Split into smaller chunks\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Check first few chunks\n",
    "for d in documents[:5]:\n",
    "    print(d.page_content[:200])  # print first 200 chars\n",
    "    print(d.metadata)\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "287c2721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}, page_content='Abhijeeth Chandragi\\nData Science Specialist — Competitive Programmer\\n# abhijeethchandragi@gmail.com | \\x83 +91 888-521-2806 | + Warangal, Telangana\\nï LinkedIn | § GitHub | Ð CodeChef: 4-star (Max 1863)\\nEducation\\nIndian Institute of Information Technology (IIIT), Nagpur\\nAug 2023 – May 2027\\nB.Tech in Data Science\\nCGPA: 8.24/10.0\\nSri Chaitanya Junior College, Telangana\\nApr 2021 – Mar 2023\\nClass 12th – Telangana State Board\\nPercentage: 98%\\nAryabhatta High School, Telangana\\nJun 2020 – Mar 2021\\nClass 10th – Telangana State Board\\nCGPA: 10.0\\nAchievements & Honors\\n• Amazon ML Summer School 2025 – Selected in the Top 2.3% nationwide (3,000 / 130,000+ applicants); completed\\n4-week intensive training on Machine Learning, Deep Learning, and Generative AI.\\n• CodeChef – 4-Star Coder – Max Rating: 1863 (Top 5% globally)\\n• Global Rank #9 in Div2 rated contest\\n• Top Finishes: #152 in Div2 rated contest #156 in Div2 rated contest\\n• Codeforces – Specialist Tier – Max Rating: 1500 (Top 15% globally)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}, page_content='• Global Rank #9 in Div2 rated contest\\n• Top Finishes: #152 in Div2 rated contest #156 in Div2 rated contest\\n• Codeforces – Specialist Tier – Max Rating: 1500 (Top 15% globally)\\n• Top Finishes: #900 (Top 3.6%) in Div2 (25,000+ participants) – #982 (Top 3.9%) in Div3 (25,000+ participants)\\n• Solved 700+ problems across 100+ contests on CodeChef, Codeforces, and other platforms.\\n• HackIndia 2025 Finalist – Secured Top 12 rank (Regional Round) among 500+ competing teams.\\nProjects\\nSmart Document Analyzer | § Code | Flask | Ollama (LLaMA3) | PyMuPDF | Sentence-Transformers\\nMay 2025\\n• Developed a document analysis backend with semantic Q&A, heading-aware chunking, and keyword-enhanced retrieval.\\n• Achieved 95%+ visual alignment by linking images to headings using layout metadata.\\n• Optimized cosine similarity and LLM prompting to reduce query latency by 30%.\\nAcademic Research Assistant | § Code | Flask | SerpAPI | LLaMA3 | MiniLM | PyPDF2\\nApr 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}, page_content='• Optimized cosine similarity and LLM prompting to reduce query latency by 30%.\\nAcademic Research Assistant | § Code | Flask | SerpAPI | LLaMA3 | MiniLM | PyPDF2\\nApr 2025\\n• Built an AI assistant to automate academic search, summarization, and metadata extraction for 200+ papers.\\n• Implemented SerpAPI for document retrieval and applied LLaMA3 + regex for structured metadata extraction.\\n• Enabled semantic ranking and summarization, reducing document review time by 60%.\\nTechnical Skills\\nProgramming Languages:\\nPython, SQL, JavaScript, C++\\nMachine Learning & AI:\\nPyTorch, TensorFlow, scikit-learn, Deep Learning, NLP, LLMs, RAG Systems\\nData Engineering:\\nPandas, NumPy, Data Preprocessing, ETL Pipelines, Power BI\\nDevelopment & Tools:\\nVS Code, Git & GitHub, Jupyter Notebooks, Linux (Ubuntu, Kali)\\nDatabases:\\nPostgreSQL, MySQL, MongoDB\\nCore Subjects:\\nData Structures & Algorithms, Computer Networks, Operating Systems, DBMS, OOP\\nPositions of Responsibility'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}, page_content='Databases:\\nPostgreSQL, MySQL, MongoDB\\nCore Subjects:\\nData Structures & Algorithms, Computer Networks, Operating Systems, DBMS, OOP\\nPositions of Responsibility\\nCore Member: Dotslash Competitive Programming Team Core member, IIIT Nagpur\\nAugust 2025 – Present')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2eb9f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_10144\\916773441.py:4: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  db = FAISS.from_documents(documents,OllamaEmbeddings(model=\"llama3\"))\n"
     ]
    }
   ],
   "source": [
    "## Vector Embeddings And Vector Store\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents,OllamaEmbeddings(model=\"llama3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d619d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Vector Database\n",
    "query = \"Projects\"\n",
    "result = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a846c212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8aae7dab-7c7b-4070-8bca-04228230c378', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}, page_content='• Optimized cosine similarity and LLM prompting to reduce query latency by 30%.\\nAcademic Research Assistant | § Code | Flask | SerpAPI | LLaMA3 | MiniLM | PyPDF2\\nApr 2025\\n• Built an AI assistant to automate academic search, summarization, and metadata extraction for 200+ papers.\\n• Implemented SerpAPI for document retrieval and applied LLaMA3 + regex for structured metadata extraction.\\n• Enabled semantic ranking and summarization, reducing document review time by 60%.\\nTechnical Skills\\nProgramming Languages:\\nPython, SQL, JavaScript, C++\\nMachine Learning & AI:\\nPyTorch, TensorFlow, scikit-learn, Deep Learning, NLP, LLMs, RAG Systems\\nData Engineering:\\nPandas, NumPy, Data Preprocessing, ETL Pipelines, Power BI\\nDevelopment & Tools:\\nVS Code, Git & GitHub, Jupyter Notebooks, Linux (Ubuntu, Kali)\\nDatabases:\\nPostgreSQL, MySQL, MongoDB\\nCore Subjects:\\nData Structures & Algorithms, Computer Networks, Operating Systems, DBMS, OOP\\nPositions of Responsibility'),\n",
       " Document(id='58f3a295-f48f-4534-a191-83b0c8efafa4', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}, page_content='Databases:\\nPostgreSQL, MySQL, MongoDB\\nCore Subjects:\\nData Structures & Algorithms, Computer Networks, Operating Systems, DBMS, OOP\\nPositions of Responsibility\\nCore Member: Dotslash Competitive Programming Team Core member, IIIT Nagpur\\nAugust 2025 – Present'),\n",
       " Document(id='fa049990-5b53-4f9b-9ca9-b00bbf985e39', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}, page_content='Abhijeeth Chandragi\\nData Science Specialist — Competitive Programmer\\n# abhijeethchandragi@gmail.com | \\x83 +91 888-521-2806 | + Warangal, Telangana\\nï LinkedIn | § GitHub | Ð CodeChef: 4-star (Max 1863)\\nEducation\\nIndian Institute of Information Technology (IIIT), Nagpur\\nAug 2023 – May 2027\\nB.Tech in Data Science\\nCGPA: 8.24/10.0\\nSri Chaitanya Junior College, Telangana\\nApr 2021 – Mar 2023\\nClass 12th – Telangana State Board\\nPercentage: 98%\\nAryabhatta High School, Telangana\\nJun 2020 – Mar 2021\\nClass 10th – Telangana State Board\\nCGPA: 10.0\\nAchievements & Honors\\n• Amazon ML Summer School 2025 – Selected in the Top 2.3% nationwide (3,000 / 130,000+ applicants); completed\\n4-week intensive training on Machine Learning, Deep Learning, and Generative AI.\\n• CodeChef – 4-Star Coder – Max Rating: 1863 (Top 5% globally)\\n• Global Rank #9 in Div2 rated contest\\n• Top Finishes: #152 in Div2 rated contest #156 in Div2 rated contest\\n• Codeforces – Specialist Tier – Max Rating: 1500 (Top 15% globally)'),\n",
       " Document(id='024ba900-1c0f-4cef-a41e-4a23225c9e60', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}, page_content='• Global Rank #9 in Div2 rated contest\\n• Top Finishes: #152 in Div2 rated contest #156 in Div2 rated contest\\n• Codeforces – Specialist Tier – Max Rating: 1500 (Top 15% globally)\\n• Top Finishes: #900 (Top 3.6%) in Div2 (25,000+ participants) – #982 (Top 3.9%) in Div3 (25,000+ participants)\\n• Solved 700+ problems across 100+ contests on CodeChef, Codeforces, and other platforms.\\n• HackIndia 2025 Finalist – Secured Top 12 rank (Regional Round) among 500+ competing teams.\\nProjects\\nSmart Document Analyzer | § Code | Flask | Ollama (LLaMA3) | PyMuPDF | Sentence-Transformers\\nMay 2025\\n• Developed a document analysis backend with semantic Q&A, heading-aware chunking, and keyword-enhanced retrieval.\\n• Achieved 95%+ visual alignment by linking images to headings using layout metadata.\\n• Optimized cosine similarity and LLM prompting to reduce query latency by 30%.\\nAcademic Research Assistant | § Code | Flask | SerpAPI | LLaMA3 | MiniLM | PyPDF2\\nApr 2025')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92d53b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='8aae7dab-7c7b-4070-8bca-04228230c378', metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-21T19:23:59+00:00', 'source': 'Resume (26).pdf', 'file_path': 'Resume (26).pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-21T19:23:59+00:00', 'trapped': '', 'modDate': 'D:20250821192359Z', 'creationDate': 'D:20250821192359Z', 'page': 0}, page_content='• Optimized cosine similarity and LLM prompting to reduce query latency by 30%.\\nAcademic Research Assistant | § Code | Flask | SerpAPI | LLaMA3 | MiniLM | PyPDF2\\nApr 2025\\n• Built an AI assistant to automate academic search, summarization, and metadata extraction for 200+ papers.\\n• Implemented SerpAPI for document retrieval and applied LLaMA3 + regex for structured metadata extraction.\\n• Enabled semantic ranking and summarization, reducing document review time by 60%.\\nTechnical Skills\\nProgramming Languages:\\nPython, SQL, JavaScript, C++\\nMachine Learning & AI:\\nPyTorch, TensorFlow, scikit-learn, Deep Learning, NLP, LLMs, RAG Systems\\nData Engineering:\\nPandas, NumPy, Data Preprocessing, ETL Pipelines, Power BI\\nDevelopment & Tools:\\nVS Code, Git & GitHub, Jupyter Notebooks, Linux (Ubuntu, Kali)\\nDatabases:\\nPostgreSQL, MySQL, MongoDB\\nCore Subjects:\\nData Structures & Algorithms, Computer Networks, Operating Systems, DBMS, OOP\\nPositions of Responsibility')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c18dd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model = \"llama3\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3c1b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answe the following query based only on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "I will tip you $1000 if users finds it helpful.\n",
    "Give a nice formated text which can directly be displayed to user.\n",
    "<context>\n",
    "{context}   \n",
    "</context> \n",
    "Question : {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c64cfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "doc_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efa95a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000027AE292B4D0>, search_kwargs={})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1528f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retreival_chain = create_retrieval_chain(retriever,doc_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "153d9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retreival_chain.invoke({\"input\":\"What are my achievements\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8f36c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Achievements & Honors**\\n\\n• **Amazon ML Summer School 2025**: Selected in the Top 2.3% nationwide (3,000 / 130,000+ applicants); completed 4-week intensive training on Machine Learning, Deep Learning, and Generative AI.\\n\\n• **CodeChef – 4-Star Coder**: Max Rating: 1863 (Top 5% globally)\\n\\n• **Global Rank #9 in Div2 rated contest**\\n\\n• **Top Finishes**:\\n\\t+ #152 in Div2 rated contest\\n\\t+ #156 in Div2 rated contest\\n\\n• **Codeforces – Specialist Tier**: Max Rating: 1500 (Top 15% globally)\\n\\t+ Top Finishes:\\n\\t\\t- #900 (Top 3.6%) in Div2 (25,000+ participants) – #982 (Top 3.9%) in Div3 (25,000+ participants)\\n\\n• **Solved 700+ problems** across 100+ contests on CodeChef, Codeforces, and other platforms.\\n\\n• **HackIndia 2025 Finalist**: Secured Top 12 rank (Regional Round) among 500+ competing teams.\\n\\nNote: These achievements are based only on the provided context.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58763c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
